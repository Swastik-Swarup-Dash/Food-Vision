# -*- coding: utf-8 -*-
"""Food_Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d6jnihT4frQlSuZQjnQjLTNgiqSwMWBJ

##Food Vision From food101paper
"""

!nvidia-smi

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

from helper_functions import create_tensorboard_callback,plot_loss_curves,unzip_data,walk_through_dir,compare_historys

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip


unzip_data("101_food_classes_10_percent.zip")

train_dir = "101_food_classes_10_percent/train/"
test_dir  = "101_food_classes_10_percent/test/"

walk_through_dir("101_food_classes_10_percent")

import tensorflow as tf
IMG_SIZE = (224,224)
BATCH_SIZE = 32

train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                          label_mode="categorical",
                                                                          image_size = IMG_SIZE)


test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                               label_mode="categorical",
                                                               image_size = IMG_SIZE,
                                                               shuffle=False)

#Train a Big Dog Model

checkpoint_path = "101_classes_10_percent-data_model_checkpoint"

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                          save_weights=True,
                                                          monitor="val_accuracy",
                                                          save_best_only = True)

## Data Augmentation
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

data_augmentation = Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomHeight(0.2),
    layers.RandomWidth(0.2)
],name="data_augmentation")

# Setup base model and freeze its layers
base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)
base_model.trainable = False


inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation(inputs)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D(name="global_average_pooling")(x)
outputs = layers.Dense(len(train_data_10_percent.class_names), activation="softmax", name="output_layer")(x)
model = tf.keras.Model(inputs, outputs)


# Compile
model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

history_all_classes_10_percent = model.fit(train_data_10_percent,
                                           epochs=5,
                                           steps_per_epoch=len(train_data_10_percent),
                                           validation_data=test_data,
                                           validation_steps=int(0.15 * len(test_data)),
                                           callbacks=[checkpoint_callback])

len(train_data_10_percent.class_names)

results_feature_extraction_model = model.evaluate(test_data)
results_feature_extraction_model

plot_loss_curves(history_all_classes_10_percent)

"""## Lets Fine Tune"""

base_model.trainable = True

for layer in base_model.layers[:-5]:
  layer.trainable = False

#Recompile
model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=["accuracy"])

for layer in model.layers:
  print(layer.name, layer.trainable)

for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name, layer.trainable)

fine_tune_epochs = 10

history_all_classes_10_percent_fine_tune = model.fit(train_data_10_percent,
                                                     epochs=fine_tune_epochs,
                                                     validation_data=test_data,
                                                     validation_steps=int(0.15 * len(test_data)), # validate on 15% of the test data
                                                     initial_epoch=history_all_classes_10_percent.epoch[-1])

results_all_classes_10_percent_fine_tune = model.evaluate(test_data)
results_all_classes_10_percent_fine_tune

plot_loss_curves(history_all_classes_10_percent_fine_tune)

compare_historys(history_all_classes_10_percent,history_all_classes_10_percent_fine_tune,5)

"""#Save The Model"""

model.save("drive/MyDrive/tensorflow_course/101_food_classes_10_percent_saved_big_dog_model")

loaded_model = tf.keras.models.load_model("drive/MyDrive/tensorflow_course/101_food_classes_10_percent_saved_big_dog_model")

loaded_model_results = loaded_model.evaluate(test_data)
results_all_classes_10_percent_fine_tune

results_all_classes_10_percent_fine_tune == results_all_classes_10_percent_fine_tune

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip

unzip_data("/content/06_101_food_class_10_percent_saved_big_dog_model.zip")

model = tf.keras.models.load_model("/content/06_101_food_class_10_percent_saved_big_dog_model")

#Evaluate Loaded model
results_downloaded_model = model.evaluate(test_data)
results_downloaded_model

pred_probs = model.predict(test_data, verbose=1)
pred_probs

len(pred_probs)

pred_probs.shape

pred_probs[:10]

pred_probs[0],len(pred_probs[0]),sum(pred_probs[0])

print(f"no of perd_probs for sample 0 : {len(pred_probs[0])}")
print(f"what pred prob sample 0 looks like:\n {pred_probs[0]}")
print(f"The class with the higher predicted probablity by the model sample for 0:\n {pred_probs[0].argmax()}")

test_data.class_names[52]

pred_classes = pred_probs.argmax(axis=1)
pred_classes[:10]

pred_classes

test_data

y_labels = []
for images,labels in test_data.unbatch():
  y_labels.append(labels.numpy().argmax())
y_labels[:10]

len(y_labels)

from sklearn.metrics import accuracy_score
sklearn_accuracy = accuracy_score(y_true=y_labels,y_pred=pred_classes)
sklearn_accuracy

import numpy as np
np.isclose(results_downloaded_model[1],sklearn_accuracy)

class_names = test_data.class_names
class_names

import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.

  Args:
    y_true: Array of truth labels (must be same shape as y_pred).
    y_pred: Array of predicted labels (must be same shape as y_true).
    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.
    figsize: Size of output figure (default=(10, 10)).
    text_size: Size of output figure text (default=15).
    norm: normalize values or not (default=False).
    savefig: save confusion matrix to file (default=False).

  Returns:
    A labelled confusion matrix plot comparing y_true and y_pred.

  Example usage:
    make_confusion_matrix(y_true=test_labels, # ground truth test labels
                          y_pred=y_preds, # predicted labels
                          classes=class_names, # array of class label names
                          figsize=(15, 15),
                          text_size=10)
  """
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes),
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)

  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  ###changed (plot x-axis vertically)
  plt.xticks(rotation=70,fontsize=text_size)
  plt.yticks(fontsize=text_size)
  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if norm:
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)
    else:
      plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
  if savefig:
    fig.savefig("confusion_matrix.png")

make_confusion_matrix(y_labels,pred_classes,class_names,figsize=(100,100),text_size=20,savefig=True)

from sklearn.metrics import classification_report
print(classification_report(y_labels,pred_classes))

classification_report_dict = classification_report(y_labels,pred_classes,output_dict=True)
classification_report_dict

classification_report_dict = classification_report(y_labels,pred_classes,output_dict=True)
classification_report_dict

#Create empty dictionary
class_f1_scores ={}
for k, v in classification_report_dict.items():
  if k == "accuracy":
    break;
  else:
    class_f1_scores[class_names[int(k)]] = v["f1-score"]

class_f1_scores

import pandas as pd
f1_scores = pd.DataFrame({"class_names": list(class_f1_scores.keys()),"f1-score": list(class_f1_scores.values())}).sort_values("f1-score",ascending=False)

f1_scores

f1_scores[10:]

fig, ax = plt.subplots(figsize=(12,25))
scores = ax.barh(range(len(f1_scores)),f1_scores["f1-score"].values)
ax.set_yticks(range(len(f1_scores)))
ax.set_yticklabels(f1_scores["class_names"])
ax.set_xlabel("F1-score")
ax.set_title("F1-scores for 101 Different Food Classes")
ax.invert_yaxis();

"""# Lets Visualize on Test_Data"""

def load_and_prep_image(filename, img_shape=224,scale=True):
  """
  Reads an image from filename, turns it into a tensor
  and reshapes it to (img_shape, img_shape, colour_channel).
  """
  # Read in target file (an image)
  img = tf.io.read_file(filename)

  # Decode the read file into a tensor & ensure 3 colour channels
  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)
  img = tf.image.decode_image(img, channels=3)

  # Resize the image (to the same size our model was trained on)
  img = tf.image.resize(img, size = [img_shape, img_shape])

  # Rescale the image (get all values between 0 and 1)
  if scale:
   return img/255.
  else:
    return img

# 1. Get the filenames of all of our test data
filepaths = []
for filepath in test_data.list_files("101_food_classes_10_percent/test/*/*.jpg",
                                     shuffle=False):
  filepaths.append(filepath.numpy())
filepaths[:10]

"""#Finding out the Wrong Prediction"""

import pandas as pd
import pandas as pd
pred_df = pd.DataFrame({"img_path": filepaths,
                        "y_true": y_labels,
                        "y_pred": pred_classes,
                        "pred_conf": pred_probs.max(axis=1), # get the maximum prediction probability value
                        "y_true_classname": [class_names[i] for i in y_labels],
                        "y_pred_classname": [class_names[i] for i in pred_classes]})
pred_df

pred_df["pred_correct"] = pred_df["y_true"] == pred_df["y_pred"]
pred_df.head()

top_100_wrong = pred_df[pred_df["pred_correct"]==False].sort_values("pred_conf", ascending=False)[:100]
top_100_wrong.head(20)

images_to_view = 9
start_index = 10  # change the start index to view more
plt.figure(figsize=(15, 10))
for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples(index=False)):
    plt.subplot(3, 3, i + 1)
    img_path = row[0]  # Assuming the image path is the first element
    img = load_and_prep_image(img_path, scale=False)

    # Extracting the required values
    pred_prob = row[1]  # Assuming predicted confidence is the second element
    y_true_classname = row[2]  # Assuming true class name is the third element
    y_pred_classname = row[3]  # Assuming predicted class name is the fourth element

    plt.imshow(img / 255.0)
    plt.title(f"actual: {y_true_classname}, pred: {y_pred_classname} \nprob: {pred_prob:.2f}")
    plt.axis(False)
plt.show()

class_names = test_data.class_names
class_names

"""#Get Custom images"""

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip
unzip_data("custom_food_images.zip")

import os
custom_food_images = ["custom_food_images/" + img_path for img_path in os.listdir("custom_food_images")]
custom_food_images

for img in custom_food_images:
  img = load_and_prep_image(img,scale=False)
  pred_prob = model.predict(tf.expand_dims(img,axis=0))
  pred_class = class_names[pred_prob.argmax()]
  plt.figure()
  plt.imshow(img/255.)
  plt.title(f"pred:{pred_classes},prob:{pred_prob.max():.2f}")
  plt.axis(False)